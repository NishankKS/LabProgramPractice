{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a379a0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  filename                                            content\n",
      "0              aditya.docx                                             aditya\n",
      "1  Ayush Aditya_Resume.pdf  EDUCATION\\n•BE in Artificial intelligence and ...\n",
      "2               sample.txt                                              ayush\n"
     ]
    }
   ],
   "source": [
    "#1a\n",
    "import pandas as pd\n",
    "import os\n",
    "import docx\n",
    "import PyPDF2\n",
    "dir_path='C:\\\\Users\\\\ayush\\\\OneDrive\\\\Desktop\\\\DSCE\\\\7\\\\nl\\\\1'\n",
    "files=[f for f in os.listdir(dir_path) if (f.endswith('.txt') or f.endswith('.docx') or f.endswith('.pdf'))]\n",
    "data = []\n",
    "for txt_file in files:\n",
    "    if(txt_file.endswith('.txt')):\n",
    "        with open(os.path.join(dir_path, txt_file), 'r') as file:\n",
    "            content = file.read()\n",
    "            data.append({'filename': txt_file, 'content': content })\n",
    "    elif(txt_file.endswith('.docx')):\n",
    "        docx_path = os.path.join(dir_path, txt_file)\n",
    "        doc = docx.Document(docx_path)\n",
    "        content = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        data.append({'filename': txt_file, 'content': content })\n",
    "    elif(txt_file.endswith('.pdf')):\n",
    "        with open(os.path.join(dir_path, txt_file), 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            for page in range(num_pages):\n",
    "                content=pdf_reader.pages[page].extract_text()\n",
    "            data.append({'filename': txt_file, 'content': content}) \n",
    "df = pd.DataFrame(data)\n",
    "print(df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1b\n",
    "import os\n",
    "\n",
    "dir_path = 'C:\\\\Users\\\\ayush\\\\OneDrive\\\\Desktop\\\\DSCE\\\\7\\\\nl\\\\1'\n",
    "files = [f for f in os.listdir(dir_path) if f.endswith('.txt')]\n",
    "data = []\n",
    "\n",
    "for txt_file in files:\n",
    "    with open(os.path.join(dir_path, txt_file), 'r') as file:\n",
    "        content = file.read()\n",
    "        data.append({'filename': txt_file, 'content': content})\n",
    "\n",
    "# Print the content of text files\n",
    "for item in data:\n",
    "    print(\"Filename:\", item['filename'])\n",
    "    print(\"Content:\")\n",
    "    print(item['content'])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3f358d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  filename                                            content  \\\n",
      "0              aditya.docx                                             aditya   \n",
      "1  Ayush Aditya_Resume.pdf  EDUCATION\\n•BE in Artificial intelligence and ...   \n",
      "2               sample.txt                                              ayush   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0                                             aditya  \n",
      "1  educ artifici intellig machin learn dayananda ...  \n",
      "2                                              ayush  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#2a\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [ps.stem(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "    df['cleaned_content'] = df['content'].apply(clean_text)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "    print(lemmatized_output)\n",
    "df['cleaned_content'] = df['content'].apply(clean_text)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2b\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample data creation (replace this with your actual data loading logic)\n",
    "data = {'content': [\"This is an example sentence.\", \"Another example sentence.\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set([\"is\", \"an\", \"the\", \"this\", \"another\"])  # Add more stopwords as needed\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Stemming (using a simple example)\n",
    "    tokens = [word[:-1] if word.endswith('s') else word for word in tokens]\n",
    "    \n",
    "    # Lemmatization (using a simple example)\n",
    "    tokens = [word[:-1] if word.endswith('s') else word for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['cleaned_content'] = df['content'].apply(clean_text)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcfe8733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  filename                                            content  \\\n",
      "0              aditya.docx                                             aditya   \n",
      "1  Ayush Aditya_Resume.pdf  EDUCATION\\n•BE in Artificial intelligence and ...   \n",
      "2               sample.txt                                              ayush   \n",
      "\n",
      "                                     cleaned_content  \\\n",
      "0                                             aditya   \n",
      "1  educ artifici intellig machin learn dayananda ...   \n",
      "2                                              ayush   \n",
      "\n",
      "                                             trigram  \n",
      "0                                                 []  \n",
      "1  [educ artifici, artifici intellig, intellig ma...  \n",
      "2                                                 []  \n"
     ]
    }
   ],
   "source": [
    "#3a\n",
    "from nltk.util import ngrams\n",
    "def generate_ngrams(text, n):\n",
    "    \"\"\"Generate n-grams from the given text\"\"\"\n",
    "    tokens = text.split()\n",
    "    return [' '.join(gram) for gram in ngrams(tokens, n)]\n",
    "df['trigram'] = df['cleaned_content'].apply(generate_ngrams, n=2)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3b\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data creation\n",
    "data = {'cleaned_content': [\"this is an example sentence\", \"another example sentence\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to generate bigrams\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    ngrams_list = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    return ngrams_list\n",
    "\n",
    "# Apply the function to generate bigrams\n",
    "df['bigrams'] = df['cleaned_content'].apply(generate_ngrams, n=2)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f658c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  filename                                            content  \\\n",
      "0              aditya.docx                                             aditya   \n",
      "1  Ayush Aditya_Resume.pdf  EDUCATION\\n•BE in Artificial intelligence and ...   \n",
      "2               sample.txt                                              ayush   \n",
      "\n",
      "                                     cleaned_content  \\\n",
      "0                                             aditya   \n",
      "1  educ artifici intellig machin learn dayananda ...   \n",
      "2                                              ayush   \n",
      "\n",
      "                                             trigram  \\\n",
      "0                                                 []   \n",
      "1  [educ artifici, artifici intellig, intellig ma...   \n",
      "2                                                 []   \n",
      "\n",
      "                                            POS_tags  \n",
      "0                                     [(aditya, NN)]  \n",
      "1  [(educ, NN), (artifici, NN), (intellig, NN), (...  \n",
      "2                                      [(ayush, NN)]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#4a\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "def pos_tagging(text):\n",
    "    \"\"\"Generate POS tags for the given text\"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return nltk.pos_tag(tokens)\n",
    "# Apply POS tagging to the cleaned_content\n",
    "df['POS_tags'] = df['cleaned_content'].apply(pos_tagging)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4b\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data creation\n",
    "data = {'cleaned_content': [\"this is an example sentence\", \"another example sentence\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to generate simple POS tags (Noun, Verb, Adjective)\n",
    "def simple_pos_tagging(text):\n",
    "    tokens = text.split()\n",
    "    pos_tags = []\n",
    "    for token in tokens:\n",
    "        if token.endswith('ing'):\n",
    "            pos_tags.append((token, 'Verb'))\n",
    "        elif token.endswith('ly'):\n",
    "            pos_tags.append((token, 'Adverb'))\n",
    "        else:\n",
    "            pos_tags.append((token, 'Noun'))\n",
    "    return pos_tags\n",
    "\n",
    "# Apply the function to generate POS tags\n",
    "df['POS_tags'] = df['cleaned_content'].apply(simple_pos_tagging)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41dcdedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  filename                                            content  \\\n",
      "0              aditya.docx                                             aditya   \n",
      "1  Ayush Aditya_Resume.pdf  EDUCATION\\n•BE in Artificial intelligence and ...   \n",
      "2               sample.txt                                              ayush   \n",
      "\n",
      "                                     cleaned_content  \\\n",
      "0                                             aditya   \n",
      "1  educ artifici intellig machin learn dayananda ...   \n",
      "2                                              ayush   \n",
      "\n",
      "                                             trigram  \\\n",
      "0                                                 []   \n",
      "1  [educ artifici, artifici intellig, intellig ma...   \n",
      "2                                                 []   \n",
      "\n",
      "                                            POS_tags  \\\n",
      "0                                     [(aditya, NN)]   \n",
      "1  [(educ, NN), (artifici, NN), (intellig, NN), (...   \n",
      "2                                      [(ayush, NN)]   \n",
      "\n",
      "                                        noun_phrases  \n",
      "0                                           [aditya]  \n",
      "1  [educ, artifici, intellig, machin, dayananda, ...  \n",
      "2                                            [ayush]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#5a\n",
    "import nltk \n",
    "nltk.download('maxent_ne_chunker') \n",
    "nltk.download('words')\n",
    "def noun_phrase_chunking(text_with_tags): \n",
    "    \"\"\" Extract noun phrases using chunking \"\"\" \n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    tree = cp.parse(text_with_tags)\n",
    "    noun_phrases = [] \n",
    "    for subtree in tree.subtrees(): \n",
    "        if subtree.label() == 'NP': \n",
    "            noun_phrases.append(' '.join(word for word, tag in subtree.leaves())) \n",
    "    return noun_phrases\n",
    "df['noun_phrases'] = df['POS_tags'].apply(noun_phrase_chunking) \n",
    "print(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5b\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data creation\n",
    "data = {'POS_tags': [\n",
    "    [('this', 'Noun'), ('is', 'Noun'), ('an', 'Noun'), ('example', 'Noun'), ('sentence', 'Noun')],\n",
    "    [('another', 'Noun'), ('example', 'Noun'), ('sentence', 'Noun')]\n",
    "]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to perform simple noun phrase chunking\n",
    "def simple_noun_phrase_chunking(pos_tags):\n",
    "    noun_phrases = []\n",
    "    current_phrase = []\n",
    "    \n",
    "    for token, tag in pos_tags:\n",
    "        if tag in ['Noun', 'Adjective']:\n",
    "            current_phrase.append(token)\n",
    "        elif current_phrase:\n",
    "            noun_phrases.append(' '.join(current_phrase))\n",
    "            current_phrase = []\n",
    "    \n",
    "    if current_phrase:\n",
    "        noun_phrases.append(' '.join(current_phrase))\n",
    "    \n",
    "    return noun_phrases\n",
    "\n",
    "# Apply the function to generate noun phrases\n",
    "df['noun_phrases'] = df['POS_tags'].apply(simple_noun_phrase_chunking)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b891bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible Completions:\n",
      "- After a long day at work, I like to relax by watching my favorite TV show\n",
      "- After a long day at work, I like to relax by going for a walk\n",
      "- After a long day at work, I like to relax by reading a book\n"
     ]
    }
   ],
   "source": [
    "#6a\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sentence prompts dictionary\n",
    "sentence_prompts = {\n",
    "    \"She opened the door and saw a\": [\"beautiful garden\", \"mysterious figure\", \"bright light\"],\n",
    "    \"After a long day at work, I like to relax by\": [\"watching my favorite TV show\", \"going for a walk\", \"reading a book\"]\n",
    "}\n",
    "\n",
    "# Input prompt\n",
    "input_prompt = \"After a long day at work, I like to relax by\"\n",
    "\n",
    "# Check if the input prompt is in the dictionary\n",
    "if input_prompt in sentence_prompts:\n",
    "    possible_completions = sentence_prompts[input_prompt]\n",
    "    print(\"Possible Completions:\")\n",
    "    for completion in possible_completions:\n",
    "        print(f\"- {input_prompt} {completion}\")\n",
    "else:\n",
    "    print(\"Prompt not found in the dictionary.\")\n",
    "    # Use spaCy to generate a random sentence completion\n",
    "    doc = nlp(input_prompt)\n",
    "    random_completion = \" \".join([token.text for token in doc] + [random.choice([\"enjoying\", \"listening\", \"playing\"])])\n",
    "    print(f\"- {random_completion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6b\n",
    "\n",
    "sentence_prompts = {\n",
    "    \"She opened the door and saw a\": [\"beautiful garden\", \"mysterious figure\", \"bright light\"],\n",
    "    \"After a long day at work, I like to relax by\": [\"watching my favorite TV show\", \"going for a walk\", \"reading a book\"]\n",
    "}\n",
    "\n",
    "input_prompt = \"After a long day at work, I like to relax by\"\n",
    "\n",
    "if input_prompt in sentence_prompts:\n",
    "    possible_completions = sentence_prompts[input_prompt]\n",
    "    print(\"Possible Completions:\")\n",
    "    for completion in possible_completions:\n",
    "        print(f\"- {input_prompt} {completion}\")\n",
    "else:\n",
    "    print(\"Prompt not found in the dictionary.\")\n",
    "    # Use random to create a random sentence completion\n",
    "    random_completion = random.choice([\"enjoying a cup of tea\", \"listening to music\", \"playing video games\"])\n",
    "    print(f\"- {input_prompt} {random_completion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7f27f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   text  sentiment_score     label\n",
      "0  I love this product!            0.625  positive\n",
      "1        It's terrible.           -1.000  negative\n",
      "2    Neutral statement.            0.000   neutral\n"
     ]
    }
   ],
   "source": [
    "#7a\n",
    "from textblob import TextBlob\n",
    "data = [\"I love this product!\", \"It's terrible.\", \"Neutral statement.\"]\n",
    "sentiments = [TextBlob(text).sentiment.polarity for text in data]\n",
    "labels = ['positive' if score > 0 else 'negative' if score < 0 else'neutral' for score in sentiments]\n",
    "result_df = pd.DataFrame({'text': data, 'sentiment_score': sentiments,'label': labels})\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7b\n",
    "\n",
    "data = [\"I love this product!\", \"It's terrible.\", \"Neutral statement.\"]\n",
    "\n",
    "def determine_sentiment_label(text):\n",
    "    if \"love\" in text.lower():\n",
    "        return 'positive'\n",
    "    elif \"terrible\" in text.lower():\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "result_dict = {'text': data, 'label': [determine_sentiment_label(text) for text in data]}\n",
    "\n",
    "for text, label in zip(result_dict['text'], result_dict['label']):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4030b927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ayush\\anaconda3\\lib\\site-packages (4.35.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: requests in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' President Trump ordered the military to start withdrawing roughly 7,000 troops from Afghanistan in the coming months, two defense officials said Thursday . The move is an abrupt shift in the 17-year-old war there and a decision that stunned Afghan officials, who said they had not been briefed on the plans . The announcement came hours after Jim Mattis, the secretary of defense, said that he would resign from his position .'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip install transformers\n",
    "pip install sentence-transformers\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, BartTokenizer, BartForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def abstractive_summarization(text):\n",
    "    # GPT-2 model for abstractive summarization\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Tokenize and generate summary\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=150, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "def extractive_summarization_sentence_transformers(text, num_sentences=3):\n",
    "    # Sentence Transformers for extractive summarization\n",
    "    model = SentenceTransformer(\"bert-base-nli-mean-tokens\")\n",
    "\n",
    "    # Split text into sentences\n",
    "    sentences = text.split('. ')\n",
    "\n",
    "    # Compute sentence embeddings\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    # Calculate pairwise cosine similarity between embeddings\n",
    "    similarity_matrix = cosine_similarity(embeddings, embeddings)\n",
    "\n",
    "    # Get indices of top-ranked sentences based on similarity\n",
    "    top_sentence_indices = np.argsort(similarity_matrix.sum(axis=1))[-num_sentences:]\n",
    "\n",
    "    # Sort sentences based on their original order\n",
    "    top_sentence_indices = sorted(top_sentence_indices)\n",
    "\n",
    "    # Generate extractive summary\n",
    "    extractive_summary = '. '.join(sentences[i] for i in top_sentence_indices)\n",
    "\n",
    "    return extractive_summary\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"In the heart of the bustling city, there stood an old bookstore with creaky woodenfloors and shelves that seemed to lean under the weight of countless stories. The air wasfilled with the comforting scent of aged paper and the soft murmur of people lost in the  worlds between the covers. A the afternoon sun streamed through dusty windows, casting a\n",
    "warm glow on antique book covers, occasionally knocking over a book or two. The bookstore, with its charm and character, was a haven for book lovers seeking solace and adventure within the pages of both old classics and new releases.\n",
    "\"\"\"\n",
    "abstractive_summary = abstractive_summarization(text)\n",
    "\n",
    "extractive_summary_sentence_transformers = extractive_summarization_sentence_transformers(text)\n",
    "\n",
    "print(\"Abstractive Summary:\", abstractive_summary)\n",
    "\n",
    "print(\"\\nExtractive Summary:\", extractive_summary_sentence_transformers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8b\n",
    "def simple_summarization(article, num_sentences=3):\n",
    "    sentences = article.split(\".\")\n",
    "    # Remove empty strings from the list\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    # Calculate the importance score for each sentence (based on sentence length)\n",
    "    scores = [len(sentence) for sentence in sentences]\n",
    "    \n",
    "    # Select the top N sentences with the highest importance scores\n",
    "    selected_sentences = sorted(zip(sentences, scores), key=lambda x: x[1], reverse=True)[:num_sentences]\n",
    "    \n",
    "    # Extract the selected sentences\n",
    "    summary = [sentence for sentence, _ in selected_sentences]\n",
    "    \n",
    "    return '. '.join(summary)\n",
    "\n",
    "# Article text\n",
    "article = \"\"\"\n",
    "    WASHINGTON - The Trump administration has ordered the military to start withdrawing roughly 7,000 troops from Afghanistan in\n",
    "    the coming months, two defense officials said Thursday, an abrupt shift in the 17-year-old war there and a decision that stunned Afghan officials, who said they had not been briefed on the plans.\n",
    "    President Trump made the decision to pull the troops - about half the number the United States has in Afghanistan now - at the same time he decided to pull American forces out of Syria, one official said.\n",
    "    The announcement came hours after Jim Mattis, the secretary of defense, said that he would resign from his position at the end of February after disagreeing with the president over his approach to policy in the Middle East.\n",
    "    The whirlwind of troop withdrawals and the resignation of Mr. Mattis leave a murky picture for what is next in the United States’ longest war, and they come as Afghanistan has been troubled by spasms of violence afflicting the capital, Kabul, and other important areas.\n",
    "    The United States has also been conducting talks with representatives\n",
    "    of the Taliban, in what officials have described as discussions that could lead to formal talks to end the conflict.\n",
    "    Senior Afghan officials and Western diplomats in Kabul woke up to the shock of the news on Friday morning, and many of them braced for chaos ahead.\n",
    "    Several Afghan officials, often in the loop on security planning and decision-making, said they had received no indication in recent days that the Americans would pull troops out.\n",
    "    The fear that Mr. Trump might take impulsive actions, however, often loomed in the background of discussions with the United States, they said.\n",
    "    They saw the abrupt decision as a further sign that voices from the ground were lacking in the debate over the war and that with Mr. Mattis’s resignation, Afghanistan had lost one of the last influential\n",
    "    voices in Washington who channeled the reality of the conflict into the White House’s deliberations.\n",
    "    The president long campaigned on bringing troops home, but in 2017, at\n",
    "    the request of Mr. Mattis, he begrudgingly pledged an additional 4,000\n",
    "    troops to the Afghan campaign to try to hasten an end to the conflict.\n",
    "    Though Pentagon officials have said the influx of forces - coupled with a more aggressive air campaign - was helping the war effort, Afghan forces continued to take nearly unsustainable levels of casualties and lose ground to the Taliban.\n",
    "    The renewed American effort in 2017 was the first step in ensuring Afghan forces could become more independent without a set timeline for\n",
    "    a withdrawal.\n",
    "    But with plans to quickly reduce the number of American troops in the country, it is unclear if the Afghans can hold their own against an increasingly aggressive Taliban.\n",
    "    Currently, American airstrikes are at levels not seen since the height\n",
    "    of the war, when tens of thousands of American troops were spread throughout the country.\n",
    "    That air support, officials say, consists mostly of propping up Afghan\n",
    "    troops while they try to hold territory from a resurgent Taliban.\n",
    "\"\"\"\n",
    "\n",
    "# Perform summarization\n",
    "summary = simple_summarization(article)\n",
    "\n",
    "# Print the summary\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea0f414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Hawaii/NNP)\n",
      "  and/CC\n",
      "  served/VBD\n",
      "  as/IN\n",
      "  the/DT\n",
      "  44th/CD\n",
      "  President/NNP\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#9a\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.chunk import ne_chunk\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "text = \"Barack Obama was born in Hawaii and served as the 44th President of the United States.\"\n",
    "words = word_tokenize(text)\n",
    "pos_tags = pos_tag(words)\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9b\n",
    "# Given sentence\n",
    "sentence = \"Barack Obama was born in Hawaii and served as the 44th President of the United States\"\n",
    "\n",
    "# Initialize lists\n",
    "person_list = []\n",
    "place_list = []\n",
    "\n",
    "# Extract entities and populate lists\n",
    "entities = sentence.split()\n",
    "for entity in entities:\n",
    "    if entity in [\"Barack\", \"Obama\"]:\n",
    "        person_list.append(entity)\n",
    "    elif entity in [\"Hawaii\", \"United\", \"States\"]:\n",
    "        place_list.append(entity)\n",
    "\n",
    "# Print the lists\n",
    "print(\"Person List:\", person_list)\n",
    "print(\"Place List:\", place_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80eb3012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs', '.']\n",
      "Stemmed words: ['the', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'the', 'lazi', 'dog', '.']\n",
      "Lemmatized words: ['The', 'quick', 'brown', 'fox', 'be', 'jump', 'over', 'the', 'lazy', 'dog', '.']\n",
      "\n",
      "\n",
      "Word: understanding\n",
      "Morphemes: ['mis', 'understanding']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "text = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
    "words = word_tokenize(text)\n",
    "porter_stemmer = PorterStemmer()\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for\n",
    "word in words]\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)\n",
    "print(\"Lemmatized words:\", lemmatized_words)\n",
    "print(\"\\n\")\n",
    "word = \"misunderstanding\"\n",
    "prefixes = [\"mis\"]\n",
    "root = \"understand\"\n",
    "suffixes = [\"ing\"]\n",
    "morphemes = []\n",
    "for prefix in prefixes:\n",
    "    if word.startswith(prefix):\n",
    "        morphemes.append(prefix)\n",
    "        word = word[len(prefix):]\n",
    "morphemes.append(word)\n",
    "print(\"Word:\", word)\n",
    "print(\"Morphemes:\", morphemes)        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10b\n",
    "def simple_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def simple_porter_stemmer(word):\n",
    "    # A simple stemming function (for illustration purposes)\n",
    "    if word.endswith(\"es\"):\n",
    "        return word[:-2]\n",
    "    elif word.endswith(\"s\"):\n",
    "        return word[:-1]\n",
    "    elif word.endswith(\"ing\"):\n",
    "        return word[:-3]\n",
    "    return word\n",
    "\n",
    "def simple_wordnet_lemmatizer(word):\n",
    "    # A simple lemmatization function (for illustration purposes)\n",
    "    if word.endswith(\"es\"):\n",
    "        return word[:-2]\n",
    "    elif word.endswith(\"s\"):\n",
    "        return word[:-1]\n",
    "    elif word.endswith(\"ing\"):\n",
    "        return word[:-3]\n",
    "    return word\n",
    "\n",
    "def analyze_morphemes(word, prefixes, root, suffixes):\n",
    "    morphemes = []\n",
    "    for prefix in prefixes:\n",
    "        if word.startswith(prefix):\n",
    "            morphemes.append(prefix)\n",
    "            word = word[len(prefix):]\n",
    "    morphemes.append(root)\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            morphemes.append(suffix)\n",
    "            word = word[:-len(suffix)]\n",
    "    return morphemes\n",
    "\n",
    "text = \"The quick brown foxes are jumping over the lazy dogs\"\n",
    "words = simple_tokenizer(text)\n",
    "\n",
    "stemmed_words = [simple_porter_stemmer(word) for word in words]\n",
    "lemmatized_words = [simple_wordnet_lemmatizer(word) for word in words]\n",
    "\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)\n",
    "print(\"Lemmatized words:\", lemmatized_words)\n",
    "print(\"\\n\")\n",
    "\n",
    "word = \"misunderstanding\"\n",
    "prefixes = [\"mis\"]\n",
    "root = \"understand\"\n",
    "suffixes = [\"ing\"]\n",
    "morphemes = analyze_morphemes(word, prefixes, root, suffixes)\n",
    "\n",
    "print(\"Word:\", word)\n",
    "print(\"Morphemes:\", morphemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddf4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
